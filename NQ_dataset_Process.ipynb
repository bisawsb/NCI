{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea079edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Any, List, Sequence, Callable\n",
    "from itertools import islice, zip_longest\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download NQ Train and Dev dataset from https://ai.google.com/research/NaturalQuestions/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev = []\n",
    "\n",
    "with open(\"v1.0-simplified_nq-dev-all.jsonl\", \"r+\", encoding=\"utf8\") as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        \n",
    "        arr = []\n",
    "        ## question_text\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        tokens = []\n",
    "        for i in item['document_tokens']:\n",
    "            tokens.append(i['token'])\n",
    "        document_text = ' '.join(tokens)\n",
    "        \n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "\n",
    "        # document_text = item['document_text']\n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        arr.append(item['document_title'])\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "        doc_tac = item['document_title'] + abs + content\n",
    "        arr.append(doc_tac)\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_dev.append(arr)\n",
    "\n",
    "nq_dev_df = pd.DataFrame(nq_dev)\n",
    "nq_dev_df.to_csv(r\"nq_dev.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699031a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train = []\n",
    "with open(\"v1.0-simplified_simplified-nq-train.jsonl\", \"r+\", encoding=\"utf8\") as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        ## question_text\n",
    "        arr = []\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "        \n",
    "        document_text = item['document_text']\n",
    "        \n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        if document_text.find('<H1>') != -1:\n",
    "            title_start = document_text.index('<H1>')\n",
    "            title_end = document_text.index('</H1>')\n",
    "            title = document_text[title_start+4:title_end]\n",
    "        else:\n",
    "            title = ''\n",
    "        arr.append(title)\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "\n",
    "        doc_tac = title + abs + content\n",
    "        arr.append(doc_tac)\n",
    "\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_train.append(arr)\n",
    "\n",
    "nq_train_df = pd.DataFrame(nq_train)\n",
    "nq_train_df.to_csv(r\"nq_train.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513ec6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## doc_tac denotes the concatenation of title, abstract and content\n",
    "\n",
    "nq_dev = pd.read_csv('nq_dev.tsv', \\\n",
    "                     names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                     header=None, sep='\\t')\n",
    "\n",
    "nq_train = pd.read_csv('nq_train.tsv', \\\n",
    "                       names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                       header=None, sep='\\t')\n",
    "\n",
    "nq_dev['title'] = nq_dev['title'].map(lower)\n",
    "nq_train['title'] = nq_train['title'].map(lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49757200",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping tool\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def lower(x):\n",
    "    text = tokenizer.tokenize(x)\n",
    "    id_ = tokenizer.convert_tokens_to_ids(text)\n",
    "    return tokenizer.decode(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat train doc and validation doc to obtain full document collection\n",
    "\n",
    "nq_all_doc = nq_train.append(nq_dev)\n",
    "nq_all_doc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicated documents based on titles\n",
    "\n",
    "nq_all_doc.drop_duplicates('title', inplace = True)\n",
    "nq_all_doc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185e620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The total amount of documents : 109739\n",
    "\n",
    "len(nq_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct mapping relation\n",
    "\n",
    "title_doc = {}\n",
    "title_doc_id = {}\n",
    "id_doc = {}\n",
    "idx = 0\n",
    "for i in range(len(nq_all_doc)):\n",
    "    title_doc[nq_all_doc['title'][i]] =  nq_all_doc['doc_tac'][i]\n",
    "    title_doc_id[nq_all_doc['title'][i]] = idx\n",
    "    id_doc[idx] = nq_all_doc['doc_tac'][i]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cce463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Document Content File\n",
    "\n",
    "train_file = open(\"NQ_doc_content.tsv\", 'w') \n",
    "\n",
    "for docid in id_doc.keys():\n",
    "    train_file.write('\\t'.join([str(docid), '', '', id_doc[docid], '', '', 'en']) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5455882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate BERT embeddings for each document\n",
    "\n",
    "id_doc_dict = {}\n",
    "train_file = \"NQ_doc_content.tsv\"\n",
    "\n",
    "with open(train_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        docid, _, _, content, _, _, _ = line.split(\"\\t\")\n",
    "        id_doc_dict[docid] = content\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "ids = list(id_doc_dict.keys())\n",
    "text_list = [id_doc_dict[id_] for id_ in ids]\n",
    "\n",
    "output_tensor = []\n",
    "output_id_tensor = []\n",
    "count = 0\n",
    "\n",
    "for elem in tqdm(text_list):\n",
    "    encoded_input = tokenizer(elem, max_length=512, return_tensors='pt', padding=True, truncation=True)\n",
    "    output = model(**encoded_input).last_hidden_state.detach().cpu()[:, 0, :].numpy().tolist()\n",
    "    output_tensor.extend(output)\n",
    "    output_id_tensor.extend(ids[count])\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"NQ_doc_content_embedding_bert_512.tsv\", 'w') \n",
    "\n",
    "for idx, doc_tensor in enumerate(output_tensor):\n",
    "    embedding = '|'.join([str(elem) for elem in doc_tensor])\n",
    "    train_file.write('\\t'.join([str(output_id_tensor[idx]), '', '', '', '', '', 'en', embedding]) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32154331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Hierarchical K-Means on it to generate semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71927b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'NQ_doc_content_embedding_bert_512.tsv',\n",
    "                 names=['docid', 'url', 'title', 'body', 'anchor', 'click', 'language', 'vector'],\n",
    "                 header=None, sep='\\t').loc[:, ['docid', 'vector']]\n",
    "df.drop_duplicates('docid', inplace = True)\n",
    "old_id = df['docid'].tolist()\n",
    "X = df['vector'].tolist()\n",
    "for idx,v in enumerate(X):\n",
    "    vec_str = v.split('|')\n",
    "    if len(vec_str) != 768:\n",
    "        print('vec dim error!')\n",
    "        exit(1)\n",
    "    X[idx] = [float(v) for v in vec_str]\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "new_id_list = []\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, max_iter=300, n_init=100, init='k-means++', random_state=7, tol=1e-7)\n",
    "\n",
    "mini_kmeans = MiniBatchKMeans(n_clusters=10, max_iter=300, n_init=100, init='k-means++', random_state=3,\n",
    "                              batch_size=1000, reassignment_ratio=0.01, max_no_improvement=20, tol=1e-7)\n",
    "\n",
    "\n",
    "def classify_recursion(x_data_pos):\n",
    "    if x_data_pos.shape[0] <= 10:\n",
    "        if x_data_pos.shape[0] == 1:\n",
    "            return\n",
    "        for idx, pos in enumerate(x_data_pos):\n",
    "            new_id_list[pos].append(idx)\n",
    "        return\n",
    "\n",
    "    temp_data = np.zeros((x_data_pos.shape[0], 768))\n",
    "    for idx, pos in enumerate(x_data_pos):\n",
    "        temp_data[idx, :] = X[pos]\n",
    "\n",
    "    if x_data_pos.shape[0] >= 1e3:\n",
    "        pred = mini_kmeans.fit_predict(temp_data)\n",
    "    else:\n",
    "        pred = kmeans.fit_predict(temp_data)\n",
    "\n",
    "    for i in range(10):\n",
    "        pos_lists = []\n",
    "        for id_, class_ in enumerate(pred):\n",
    "            if class_ == i:\n",
    "                pos_lists.append(x_data_pos[id_])\n",
    "                new_id_list[x_data_pos[id_]].append(i)\n",
    "        classify_recursion(np.array(pos_lists))\n",
    "\n",
    "    return\n",
    "\n",
    "print('Start First Clustering')\n",
    "pred = mini_kmeans.fit_predict(X)\n",
    "print(pred.shape)   #int 0-9 for each vector\n",
    "print(mini_kmeans.n_iter_)\n",
    "\n",
    "for class_ in pred:\n",
    "    new_id_list.append([class_])\n",
    "\n",
    "print('Start Recursively Clustering...')\n",
    "for i in range(10):\n",
    "    print(i, \"th cluster\")\n",
    "    pos_lists = [];\n",
    "    for id_, class_ in enumerate(pred):\n",
    "        if class_ == i:\n",
    "            pos_lists.append(id_)\n",
    "    classify_recursion(np.array(pos_lists))\n",
    "\n",
    "mapping = {}\n",
    "for i in range(len(old_id)):\n",
    "    mapping[old_id[i]] = new_id_list[i]\n",
    "\n",
    "with open('IDMapping_NQ_bert_768_k10_c10.pkl', 'wb') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('IDMapping_NQ_bert_768_k10_c10.pkl', 'rb') as f:\n",
    "    kmeans_nq_doc_dict = pickle.load(f)\n",
    "\n",
    "new_kmeans_nq_doc_dict = {}\n",
    "\n",
    "for old_docid in kmeans_nq_doc_dict.keys():\n",
    "    new_kmeans_nq_doc_dict[str(old_docid)] = ''.join(str(elem) for elem in kmeans_nq_doc_dict[old_docid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79effc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query Generation\n",
    "\n",
    "def pad_sequence_to_length(\n",
    "        sequence: Sequence,\n",
    "        desired_length: int,\n",
    "        default_value: Callable[[], Any] = lambda: 0,\n",
    "        padding_on_right: bool = True,\n",
    ") -> List:\n",
    "    sequence = list(sequence)\n",
    "    # Truncates the sequence to the desired length.\n",
    "    if padding_on_right:\n",
    "        padded_sequence = sequence[:desired_length]\n",
    "    else:\n",
    "        padded_sequence = sequence[-desired_length:]\n",
    "    # Continues to pad with default_value() until we reach the desired length.\n",
    "    pad_length = desired_length - len(padded_sequence)\n",
    "    # This just creates the default value once, so if it's a list, and if it gets mutated\n",
    "    # later, it could cause subtle bugs. But the risk there is low, and this is much faster.\n",
    "    values_to_pad = [default_value()] * pad_length\n",
    "    if padding_on_right:\n",
    "        padded_sequence = padded_sequence + values_to_pad\n",
    "    else:\n",
    "        padded_sequence = values_to_pad + padded_sequence\n",
    "    return padded_sequence\n",
    "\n",
    "qg_file = open(\"NQ_doc_no_qg.tsv\", 'w') \n",
    "\n",
    "for docid in id_doc.keys():\n",
    "    qg_file.write('\\t'.join([str(docid), new_kmeans_nq_doc_dict[str(docid)], id_doc[docid]]) + '\\n')\n",
    "    qg_file.flush()\n",
    "\n",
    "device=torch.device(\"cuda\")\n",
    "##  You can also download from Hugging Face. This folder should be in the same path as the notebook.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"castorini/doc2query-t5-base-msmarco\"\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model=torch.nn.DataParallel(model, device_ids=[0,1]).cuda()\n",
    "    \n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"doc2query-t5-base-msmarco\"\n",
    ")\n",
    "\n",
    "## format: {Did, new_did, doc_content}\n",
    "need_qg = 'NQ_doc_no_qg.tsv'\n",
    "## format: {did, qg, qid(qg_{did}_i)}\n",
    "qg_save = \"NQ_doc_qg.tsv\"\n",
    "\n",
    "did_list = []\n",
    "qg_list = []\n",
    "qid_list = []\n",
    "with open(qg_save, \"a\", newline='') as fw:\n",
    "\n",
    "    need_qg_file = pd.read_csv(need_qg, names=['Did', 'did' ,'content'], header=None, sep='\\t')\n",
    "\n",
    "    for i in trange(len(need_qg_file)):\n",
    "        next_n_lines = need_qg_file['content'][i]\n",
    "        batch_input_ids = []\n",
    "        sen = next_n_lines[:512]\n",
    "        batch_input_ids.append(tokenizer.encode(text=sen, add_special_tokens=True))\n",
    "\n",
    "        max_len = max([len(sen) for sen in batch_input_ids] )\n",
    "        batch_input_ids = [\n",
    "            pad_sequence_to_length(\n",
    "                sequence=sen, desired_length=max_len, default_value=lambda : tokenizer.pad_token_id,\n",
    "                padding_on_right=False\n",
    "            ) for sen in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        batch_input_ids = torch.tensor(data=batch_input_ids, dtype=torch.int64, device=device)\n",
    "\n",
    "        generated = model.module.generate(\n",
    "            input_ids=batch_input_ids,\n",
    "            max_length=32,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=10,\n",
    "        )\n",
    "\n",
    "        generated = tokenizer.batch_decode(sequences=generated.tolist(), skip_special_tokens=True)\n",
    "\n",
    "        for index, g in enumerate(generated):\n",
    "            fw.write('\\t'.join([g, need_qg_file['Did'][i], need_qg_file['did'][i]]) + '\\n')\n",
    "            fw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a48d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace Original IDs with Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d708418",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev = pd.read_csv('nq_dev.tsv', \\\n",
    "                     names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                     header=None, sep='\\t')\n",
    "\n",
    "nq_train = pd.read_csv('nq_train.tsv', \\\n",
    "                       names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                       header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train['oldid'] = nq_train['title'].map(title_doc_id)\n",
    "nq_train['newid'] = nq_train['oldid'].map(new_kmeans_nq_doc_dict)\n",
    "nq_train = nq_train.loc[:, ['query', 'id', 'newid']]  \n",
    "nq_train.to_csv('nq_train_doc_newid.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e60d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev['oldid'] = nq_dev['title'].map(title_doc_id)\n",
    "nq_dev['newid'] = nq_dev['oldid'].map(new_kmeans_nq_doc_dict)\n",
    "nq_dev = nq_dev.loc[:, ['query', 'id', 'newid']]  \n",
    "nq_dev.to_csv('nq_dev_doc_newid.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
